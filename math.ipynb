{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learns to solve simple math problems involving two ten-digit numbers.\n",
    "My goal at the beginning was to have it do multiplication.\n",
    "I had problems early on where it refused to converge at all. So I worked forwards starting on really easy problems.\n",
    "It had no trouble converging when the problem was, \"Take the first number and send it back out\". That confirmed that I at\n",
    "least had written the learning and question-generation correctly. I moved up to \"Take the first number and add three\".\n",
    "That worked too. Then I had it add a larger number (which would involve more carrying). I realized that the problem might\n",
    "be that I hadn't given it enough nodes to properly implement the logic. Once I upgraded the network from two 200-node\n",
    "hidden layers to 2000 and 500, it was able to solve the problem of adding the two numbers together with good accuracy. I\n",
    "still haven't gotten it to do multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "NUM_SIZE = 10\n",
    "FIRST_LAYER_SIZE = 1300\n",
    "EXTRA_LAYER_SIZE = 320\n",
    "EXTRA_LAYERS = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom layers\n",
    "\n",
    "# This is like the Softmax function, but it does it independently on each digit in the result\n",
    "# So each group of ten nodes should sum up to one\n",
    "class TenSoftmax(keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(TenSoftmax, self).__init__(**kwargs)\n",
    "        self.softmax = keras.layers.Softmax()\n",
    "    \n",
    "    def call(self, tensor, training=True):\n",
    "        s0, s1, s2, s3, s4, s5, s6, s7, s8, s9 = tf.split(tensor, num_or_size_splits=10, axis=1)\n",
    "        list = [s0, s1, s2, s3, s4, s5, s6, s7, s8, s9]\n",
    "        for i in range(10):\n",
    "            list[i] = self.softmax(list[1])\n",
    "        ret = tf.concat(list, axis=1)\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 600)               120600    \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 600)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 180)               108180    \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 180)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 180)               32580     \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 180)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 180)               32580     \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 180)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 180)               32580     \n",
      "                                                                 \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 180)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 180)               32580     \n",
      "                                                                 \n",
      " leaky_re_lu_5 (LeakyReLU)   (None, 180)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 180)               32580     \n",
      "                                                                 \n",
      " leaky_re_lu_6 (LeakyReLU)   (None, 180)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 100)               18100     \n",
      "                                                                 \n",
      " leaky_re_lu_7 (LeakyReLU)   (None, 100)               0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 409,780\n",
      "Trainable params: 409,780\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "def math_model():\n",
    "    # They happen in a linear order\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    # Input layer / First Hidden Layer\n",
    "    model.add(keras.layers.Dense(FIRST_LAYER_SIZE, input_shape=(2*10*NUM_SIZE,)))\n",
    "    model.add(keras.layers.LeakyReLU())\n",
    "\n",
    "    # Carry Layers\n",
    "    for _ in range(EXTRA_LAYERS):\n",
    "        model.add(keras.layers.Dense(EXTRA_LAYER_SIZE))\n",
    "        model.add(keras.layers.LeakyReLU())\n",
    "\n",
    "    # Output Layer\n",
    "    model.add(keras.layers.Dense(10*NUM_SIZE))\n",
    "    model.add(keras.layers.LeakyReLU())\n",
    "    # model.add(TenSoftmax())\n",
    "    \n",
    "    # Print summary\n",
    "    print(model.summary())\n",
    "\n",
    "    # Return\n",
    "    return model\n",
    "\n",
    "# Get the model\n",
    "model = math_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method returns a helper function to compute cross entropy loss\n",
    "loss_function = tf.keras.losses.MeanSquaredError()    \n",
    "# loss_function = tf.keras.losses.BinaryCrossentropy(from_logits=True)    \n",
    "# loss_function = tf.keras.losses.CategoricalCrossentropy()    \n",
    "\n",
    "# Declare optimizer (Use Adam optimizer w/ learning rate of 1e-4)\n",
    "optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(count):\n",
    "    input_data, output_data = generate_data(count)\n",
    "    output = model(input_data, training=True)\n",
    "\n",
    "    # print(output_data[0])\n",
    "    # print(output[0])\n",
    "\n",
    "    # Iterate over answers\n",
    "    correct_answers = 0\n",
    "    wrongness = 0\n",
    "    for yh, y in zip(output, output_data):\n",
    "        yhr = tf.reshape(yh, (10, 10))\n",
    "        yr = tf.reshape(y, (10, 10))\n",
    "        correct = True\n",
    "        for n1, n2 in zip(yhr, yr):\n",
    "            if np.argmax(n1) != np.argmax(n2):\n",
    "                correct = False\n",
    "                break\n",
    "        if correct:\n",
    "            correct_answers += 1\n",
    "\n",
    "    wrongness = loss_function(output, output_data)\n",
    "    \n",
    "    return correct_answers, wrongness\n",
    "\n",
    "def get_digit(number, digit):\n",
    "    number = number % (10**(digit+1))\n",
    "    number = int(number / (10**digit))\n",
    "    return number\n",
    "\n",
    "def digit_vector(num):\n",
    "    e = np.zeros([10])\n",
    "    e[num] = 1\n",
    "    return e\n",
    "\n",
    "def generate_data(data_points):\n",
    "    entries_input = []\n",
    "    entries_output = []\n",
    "    for i in range(data_points):\n",
    "        # Generate numbers\n",
    "        scale = 10**10\n",
    "        num1 = int(random.random() * scale)\n",
    "        num2 = int(random.random() * scale)\n",
    "        answer = num1 + num2\n",
    "\n",
    "        # print(num1)\n",
    "        # print(num2)\n",
    "        # print(answer)\n",
    "\n",
    "        # Convert to vectors\n",
    "        entry_input = []\n",
    "        for i in range(9, -1, -1):\n",
    "            entry_input.extend(digit_vector(get_digit(num1, i)))\n",
    "        for i in range(9, -1, -1):\n",
    "            entry_input.extend(digit_vector(get_digit(num2, i)))\n",
    "        entry_output = []\n",
    "        for i in range(9, -1, -1):\n",
    "            entry_output.extend(digit_vector(get_digit(answer, i)))\n",
    "        \n",
    "        # Append\n",
    "        entries_input.append(entry_input)\n",
    "        entries_output.append(entry_output)\n",
    "    \n",
    "    ret_input = tf.Variable(entries_input, tf.float64)\n",
    "    ret_output = tf.Variable(entries_output, tf.float64)\n",
    "\n",
    "    # print (ret_input)\n",
    "    # print (ret_output)\n",
    "    \n",
    "    return ret_input, ret_output\n",
    "\n",
    "# generate_data(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.function annotation causes the function \n",
    "# to be \"compiled\" as part of the training\n",
    "@tf.function\n",
    "def train_step(input_data, output_data):\n",
    "    # Set up tape\n",
    "    with tf.GradientTape() as tape:\n",
    "      output = model(input_data, training=True)\n",
    "\n",
    "      loss = loss_function(output_data, output)\n",
    "\n",
    "      gradients = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "      optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    # Evaluate\n",
    "    #input_data_test, output_data_test = generate_data(100)\n",
    "    #output = model(input_data_test, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 0/10\n",
      "Wrongness: 0.0362094120746065\n",
      "\n",
      "Epoch 7: 0/10\n",
      "Wrongness: 0.034823676032211835\n",
      "\n",
      "Epoch 11: 0/10\n",
      "Wrongness: 0.03373373498602531\n",
      "\n",
      "Epoch 15: 0/10\n",
      "Wrongness: 0.033116980742068515\n",
      "\n",
      "Epoch 19: 0/10\n",
      "Wrongness: 0.032159669721988825\n",
      "\n",
      "Epoch 23: 0/10\n",
      "Wrongness: 0.03149754871932382\n",
      "\n",
      "Epoch 27: 0/10\n",
      "Wrongness: 0.03099528453795543\n",
      "\n",
      "Epoch 31: 0/10\n",
      "Wrongness: 0.02806747234205261\n",
      "\n",
      "Epoch 35: 0/10\n",
      "Wrongness: 0.028993783041808496\n",
      "\n",
      "Epoch 39: 0/10\n",
      "Wrongness: 0.029384416764575438\n",
      "\n",
      "Epoch 43: 0/10\n",
      "Wrongness: 0.02803552218640145\n",
      "\n",
      "Epoch 47: 0/10\n",
      "Wrongness: 0.027064002023625217\n",
      "\n",
      "Epoch 51: 0/10\n",
      "Wrongness: 0.027562401978705502\n",
      "\n",
      "Epoch 55: 0/10\n",
      "Wrongness: 0.025523081392345704\n",
      "\n",
      "Epoch 59: 0/10\n",
      "Wrongness: 0.025862544572343882\n",
      "\n",
      "Epoch 63: 0/10\n",
      "Wrongness: 0.02478000957586492\n",
      "\n",
      "Epoch 67: 2/10\n",
      "Wrongness: 0.02303531876911712\n",
      "\n",
      "Epoch 71: 0/10\n",
      "Wrongness: 0.02595808501113963\n",
      "\n",
      "Epoch 75: 0/10\n",
      "Wrongness: 0.023409253040823044\n",
      "\n",
      "Epoch 79: 0/10\n",
      "Wrongness: 0.02317215896986467\n",
      "\n",
      "Epoch 83: 0/10\n",
      "Wrongness: 0.023473736845326547\n",
      "\n",
      "Epoch 87: 0/10\n",
      "Wrongness: 0.022006443666504956\n",
      "\n",
      "Epoch 91: 1/10\n",
      "Wrongness: 0.020508275959896204\n",
      "\n",
      "Epoch 95: 0/10\n",
      "Wrongness: 0.018923184195353927\n",
      "\n",
      "Epoch 99: 0/10\n",
      "Wrongness: 0.019276633507285174\n",
      "\n",
      "Epoch 103: 2/10\n",
      "Wrongness: 0.018366203789828707\n",
      "\n",
      "Epoch 107: 0/10\n",
      "Wrongness: 0.01886110772969759\n",
      "\n",
      "Epoch 111: 0/10\n",
      "Wrongness: 0.019088773976674328\n",
      "\n",
      "Epoch 115: 1/10\n",
      "Wrongness: 0.01693489327081004\n",
      "\n",
      "Epoch 119: 0/10\n",
      "Wrongness: 0.01761405557807714\n",
      "\n",
      "Epoch 123: 0/10\n",
      "Wrongness: 0.018038039167069114\n",
      "\n",
      "Epoch 127: 0/10\n",
      "Wrongness: 0.017550065740272132\n",
      "\n",
      "Epoch 131: 0/10\n",
      "Wrongness: 0.0166308182014253\n",
      "\n",
      "Epoch 135: 1/10\n",
      "Wrongness: 0.015745405498721214\n",
      "\n",
      "Epoch 139: 0/10\n",
      "Wrongness: 0.015287257120628988\n",
      "\n",
      "Epoch 143: 2/10\n",
      "Wrongness: 0.016028097727253818\n",
      "\n",
      "Epoch 147: 2/10\n",
      "Wrongness: 0.01535414699959724\n",
      "\n",
      "Epoch 151: 1/10\n",
      "Wrongness: 0.014571734191689454\n",
      "\n",
      "Epoch 155: 2/10\n",
      "Wrongness: 0.015065476941089354\n",
      "\n",
      "Epoch 159: 0/10\n",
      "Wrongness: 0.01538459716118172\n",
      "\n",
      "Epoch 163: 2/10\n",
      "Wrongness: 0.013106871962688926\n",
      "\n",
      "Epoch 167: 2/10\n",
      "Wrongness: 0.014448318258068731\n",
      "\n",
      "Epoch 171: 3/10\n",
      "Wrongness: 0.014321248137620974\n",
      "\n",
      "Epoch 175: 3/10\n",
      "Wrongness: 0.013340219334294589\n",
      "\n",
      "Epoch 179: 1/10\n",
      "Wrongness: 0.015240871331633215\n",
      "\n",
      "Epoch 183: 4/10\n",
      "Wrongness: 0.013123852536958889\n",
      "\n",
      "Epoch 187: 2/10\n",
      "Wrongness: 0.013711420634570972\n",
      "\n",
      "Epoch 191: 3/10\n",
      "Wrongness: 0.01209641409059817\n",
      "\n",
      "Epoch 195: 4/10\n",
      "Wrongness: 0.012301313056566711\n",
      "\n",
      "Epoch 199: 4/10\n",
      "Wrongness: 0.011191683035744786\n",
      "\n",
      "Epoch 203: 3/10\n",
      "Wrongness: 0.011752558935018313\n",
      "\n",
      "Epoch 207: 4/10\n",
      "Wrongness: 0.012141170292296544\n",
      "\n",
      "Epoch 211: 5/10\n",
      "Wrongness: 0.010328993508071519\n",
      "\n",
      "Epoch 215: 1/10\n",
      "Wrongness: 0.011839814062546881\n",
      "\n",
      "Epoch 219: 5/10\n",
      "Wrongness: 0.01022136236132836\n",
      "\n",
      "Epoch 223: 5/10\n",
      "Wrongness: 0.011536690062453014\n",
      "\n",
      "Epoch 227: 4/10\n",
      "Wrongness: 0.009332830997098338\n",
      "\n",
      "Epoch 231: 3/10\n",
      "Wrongness: 0.010758057272473132\n",
      "\n",
      "Epoch 235: 5/10\n",
      "Wrongness: 0.009652029879170802\n",
      "\n",
      "Epoch 239: 6/10\n",
      "Wrongness: 0.009745387245888687\n",
      "\n",
      "Epoch 243: 5/10\n",
      "Wrongness: 0.00843761981005455\n",
      "\n",
      "Epoch 247: 5/10\n",
      "Wrongness: 0.009196049856218306\n",
      "\n",
      "Epoch 251: 4/10\n",
      "Wrongness: 0.008567178892110216\n",
      "\n",
      "Epoch 255: 6/10\n",
      "Wrongness: 0.008889535990041268\n",
      "\n",
      "Epoch 259: 5/10\n",
      "Wrongness: 0.00818492877100464\n",
      "\n",
      "Epoch 263: 8/10\n",
      "Wrongness: 0.0075164283728697545\n",
      "\n",
      "Epoch 267: 4/10\n",
      "Wrongness: 0.00809840148975887\n",
      "\n",
      "Epoch 271: 5/10\n",
      "Wrongness: 0.008515337649779835\n",
      "\n",
      "Epoch 275: 6/10\n",
      "Wrongness: 0.007895781220144388\n",
      "\n",
      "Epoch 279: 3/10\n",
      "Wrongness: 0.008545489888037728\n",
      "\n",
      "Epoch 283: 5/10\n",
      "Wrongness: 0.009026015889141283\n",
      "\n",
      "Epoch 287: 7/10\n",
      "Wrongness: 0.008574368798989731\n",
      "\n",
      "Epoch 291: 7/10\n",
      "Wrongness: 0.00792765898146305\n",
      "\n",
      "Epoch 295: 8/10\n",
      "Wrongness: 0.0075141923623920776\n",
      "\n",
      "Epoch 299: 7/10\n",
      "Wrongness: 0.008361581879469743\n",
      "\n",
      "Epoch 303: 7/10\n",
      "Wrongness: 0.007020320492702685\n",
      "\n",
      "Epoch 307: 5/10\n",
      "Wrongness: 0.007233161700055557\n",
      "\n",
      "Epoch 311: 9/10\n",
      "Wrongness: 0.006970979761045141\n",
      "\n",
      "Epoch 315: 10/10\n",
      "Wrongness: 0.005825460289914264\n",
      "Hit 100% accuracy on 10 problems after 315 epochs!\n",
      "\n",
      "Epoch 329: 80/100\n",
      "Wrongness: 0.005841698927218437\n",
      "\n",
      "Epoch 343: 86/100\n",
      "Wrongness: 0.005190229742396338\n",
      "\n",
      "Epoch 357: 96/100\n",
      "Wrongness: 0.004421619029013169\n",
      "\n",
      "Epoch 370: 95/100\n",
      "Wrongness: 0.003993716479596868\n",
      "\n",
      "Epoch 384: 97/100\n",
      "Wrongness: 0.0034073080675431056\n",
      "\n",
      "Epoch 398: 98/100\n",
      "Wrongness: 0.0024167326882566682\n",
      "\n",
      "Epoch 411: 99/100\n",
      "Wrongness: 0.0019426879964951054\n",
      "\n",
      "Epoch 425: 99/100\n",
      "Wrongness: 0.0014787301333431613\n",
      "\n",
      "Epoch 439: 100/100\n",
      "Wrongness: 0.0008905659049218421\n",
      "Hit 100% accuracy on 100 problems after 439 epochs!\n",
      "\n",
      "Epoch 494: 999/1000\n",
      "Wrongness: 0.0005248912583905399\n",
      "\n",
      "Epoch 544: 1000/1000\n",
      "Wrongness: 0.0003815107774244156\n",
      "Hit 100% accuracy on 1000 problems after 544 epochs!\n",
      "\n",
      "Epoch 759: 10000/10000\n",
      "Wrongness: 0.00012209867385539285\n",
      "Hit 100% accuracy on 10000 problems after 759 epochs!\n",
      "\n",
      "Epoch 1606: 100000/100000\n",
      "Wrongness: 4.669437193963463e-05\n",
      "Hit 100% accuracy on 100000 problems after 1606 epochs!\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 30\u001b[0m\n\u001b[0;32m     25\u001b[0m         seconds_per_test \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m \u001b[39m4\u001b[39m\n\u001b[0;32m     27\u001b[0m       \u001b[39mprint\u001b[39m()\n\u001b[1;32m---> 30\u001b[0m train(\u001b[39m1000000\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[8], line 11\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(epochs)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m      7\u001b[0m   \n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m   \u001b[39m# For each minibatch...\u001b[39;00m\n\u001b[0;32m     10\u001b[0m   \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m100\u001b[39m):\n\u001b[1;32m---> 11\u001b[0m     input_data, output_data \u001b[39m=\u001b[39m generate_data(\u001b[39m100\u001b[39;49m)\n\u001b[0;32m     12\u001b[0m     train_step(input_data, output_data)\n\u001b[0;32m     14\u001b[0m   \u001b[39m# Test every n seconds\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 64\u001b[0m, in \u001b[0;36mgenerate_data\u001b[1;34m(data_points)\u001b[0m\n\u001b[0;32m     61\u001b[0m     entries_input\u001b[39m.\u001b[39mappend(entry_input)\n\u001b[0;32m     62\u001b[0m     entries_output\u001b[39m.\u001b[39mappend(entry_output)\n\u001b[1;32m---> 64\u001b[0m ret_input \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mVariable(entries_input, tf\u001b[39m.\u001b[39;49mfloat64)\n\u001b[0;32m     65\u001b[0m ret_output \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mVariable(entries_output, tf\u001b[39m.\u001b[39mfloat64)\n\u001b[0;32m     67\u001b[0m \u001b[39m# print (ret_input)\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[39m# print (ret_output)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py:266\u001b[0m, in \u001b[0;36mVariableMetaclass.__call__\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    264\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_variable_v1_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    265\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m Variable:\n\u001b[1;32m--> 266\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_variable_v2_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    267\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    268\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m(VariableMetaclass, \u001b[39mcls\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py:247\u001b[0m, in \u001b[0;36mVariableMetaclass._variable_v2_call\u001b[1;34m(cls, initial_value, trainable, validate_shape, caching_device, name, variable_def, dtype, import_scope, constraint, synchronization, aggregation, shape)\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[39mif\u001b[39;00m aggregation \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    246\u001b[0m   aggregation \u001b[39m=\u001b[39m VariableAggregation\u001b[39m.\u001b[39mNONE\n\u001b[1;32m--> 247\u001b[0m \u001b[39mreturn\u001b[39;00m previous_getter(\n\u001b[0;32m    248\u001b[0m     initial_value\u001b[39m=\u001b[39;49minitial_value,\n\u001b[0;32m    249\u001b[0m     trainable\u001b[39m=\u001b[39;49mtrainable,\n\u001b[0;32m    250\u001b[0m     validate_shape\u001b[39m=\u001b[39;49mvalidate_shape,\n\u001b[0;32m    251\u001b[0m     caching_device\u001b[39m=\u001b[39;49mcaching_device,\n\u001b[0;32m    252\u001b[0m     name\u001b[39m=\u001b[39;49mname,\n\u001b[0;32m    253\u001b[0m     variable_def\u001b[39m=\u001b[39;49mvariable_def,\n\u001b[0;32m    254\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m    255\u001b[0m     import_scope\u001b[39m=\u001b[39;49mimport_scope,\n\u001b[0;32m    256\u001b[0m     constraint\u001b[39m=\u001b[39;49mconstraint,\n\u001b[0;32m    257\u001b[0m     synchronization\u001b[39m=\u001b[39;49msynchronization,\n\u001b[0;32m    258\u001b[0m     aggregation\u001b[39m=\u001b[39;49maggregation,\n\u001b[0;32m    259\u001b[0m     shape\u001b[39m=\u001b[39;49mshape)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py:240\u001b[0m, in \u001b[0;36mVariableMetaclass._variable_v2_call.<locals>.<lambda>\u001b[1;34m(**kws)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_variable_v2_call\u001b[39m(\u001b[39mcls\u001b[39m,\n\u001b[0;32m    227\u001b[0m                       initial_value\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    228\u001b[0m                       trainable\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    237\u001b[0m                       aggregation\u001b[39m=\u001b[39mVariableAggregation\u001b[39m.\u001b[39mNONE,\n\u001b[0;32m    238\u001b[0m                       shape\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    239\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Call on Variable class. Useful to force the signature.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 240\u001b[0m   previous_getter \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkws: default_variable_creator_v2(\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkws)\n\u001b[0;32m    241\u001b[0m   \u001b[39mfor\u001b[39;00m _, getter \u001b[39min\u001b[39;00m ops\u001b[39m.\u001b[39mget_default_graph()\u001b[39m.\u001b[39m_variable_creator_stack:  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    242\u001b[0m     previous_getter \u001b[39m=\u001b[39m _make_getter(getter, previous_getter)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py:2755\u001b[0m, in \u001b[0;36mdefault_variable_creator_v2\u001b[1;34m(next_creator, **kwargs)\u001b[0m\n\u001b[0;32m   2752\u001b[0m aggregation \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39maggregation\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m   2753\u001b[0m shape \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mshape\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m-> 2755\u001b[0m \u001b[39mreturn\u001b[39;00m resource_variable_ops\u001b[39m.\u001b[39;49mResourceVariable(\n\u001b[0;32m   2756\u001b[0m     initial_value\u001b[39m=\u001b[39;49minitial_value,\n\u001b[0;32m   2757\u001b[0m     trainable\u001b[39m=\u001b[39;49mtrainable,\n\u001b[0;32m   2758\u001b[0m     validate_shape\u001b[39m=\u001b[39;49mvalidate_shape,\n\u001b[0;32m   2759\u001b[0m     caching_device\u001b[39m=\u001b[39;49mcaching_device,\n\u001b[0;32m   2760\u001b[0m     name\u001b[39m=\u001b[39;49mname,\n\u001b[0;32m   2761\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m   2762\u001b[0m     constraint\u001b[39m=\u001b[39;49mconstraint,\n\u001b[0;32m   2763\u001b[0m     variable_def\u001b[39m=\u001b[39;49mvariable_def,\n\u001b[0;32m   2764\u001b[0m     import_scope\u001b[39m=\u001b[39;49mimport_scope,\n\u001b[0;32m   2765\u001b[0m     distribute_strategy\u001b[39m=\u001b[39;49mdistribute_strategy,\n\u001b[0;32m   2766\u001b[0m     synchronization\u001b[39m=\u001b[39;49msynchronization,\n\u001b[0;32m   2767\u001b[0m     aggregation\u001b[39m=\u001b[39;49maggregation,\n\u001b[0;32m   2768\u001b[0m     shape\u001b[39m=\u001b[39;49mshape)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py:268\u001b[0m, in \u001b[0;36mVariableMetaclass.__call__\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    266\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_variable_v2_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    267\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 268\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m(VariableMetaclass, \u001b[39mcls\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:1659\u001b[0m, in \u001b[0;36mResourceVariable.__init__\u001b[1;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy, synchronization, aggregation, shape)\u001b[0m\n\u001b[0;32m   1656\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_from_proto(variable_def, import_scope\u001b[39m=\u001b[39mimport_scope,\n\u001b[0;32m   1657\u001b[0m                         validate_shape\u001b[39m=\u001b[39mvalidate_shape)\n\u001b[0;32m   1658\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1659\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_init_from_args(\n\u001b[0;32m   1660\u001b[0m       initial_value\u001b[39m=\u001b[39;49minitial_value,\n\u001b[0;32m   1661\u001b[0m       trainable\u001b[39m=\u001b[39;49mtrainable,\n\u001b[0;32m   1662\u001b[0m       collections\u001b[39m=\u001b[39;49mcollections,\n\u001b[0;32m   1663\u001b[0m       caching_device\u001b[39m=\u001b[39;49mcaching_device,\n\u001b[0;32m   1664\u001b[0m       name\u001b[39m=\u001b[39;49mname,\n\u001b[0;32m   1665\u001b[0m       dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m   1666\u001b[0m       constraint\u001b[39m=\u001b[39;49mconstraint,\n\u001b[0;32m   1667\u001b[0m       synchronization\u001b[39m=\u001b[39;49msynchronization,\n\u001b[0;32m   1668\u001b[0m       aggregation\u001b[39m=\u001b[39;49maggregation,\n\u001b[0;32m   1669\u001b[0m       shape\u001b[39m=\u001b[39;49mshape,\n\u001b[0;32m   1670\u001b[0m       distribute_strategy\u001b[39m=\u001b[39;49mdistribute_strategy,\n\u001b[0;32m   1671\u001b[0m       validate_shape\u001b[39m=\u001b[39;49mvalidate_shape,\n\u001b[0;32m   1672\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:1827\u001b[0m, in \u001b[0;36mResourceVariable._init_from_args\u001b[1;34m(self, initial_value, trainable, collections, caching_device, name, dtype, constraint, synchronization, aggregation, distribute_strategy, shape, validate_shape)\u001b[0m\n\u001b[0;32m   1825\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1826\u001b[0m     shape \u001b[39m=\u001b[39m initial_value\u001b[39m.\u001b[39mshape\n\u001b[1;32m-> 1827\u001b[0m   handle \u001b[39m=\u001b[39m eager_safe_variable_handle(\n\u001b[0;32m   1828\u001b[0m       initial_value\u001b[39m=\u001b[39;49minitial_value,\n\u001b[0;32m   1829\u001b[0m       shape\u001b[39m=\u001b[39;49mshape,\n\u001b[0;32m   1830\u001b[0m       shared_name\u001b[39m=\u001b[39;49mshared_name,\n\u001b[0;32m   1831\u001b[0m       name\u001b[39m=\u001b[39;49mname,\n\u001b[0;32m   1832\u001b[0m       graph_mode\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_in_graph_mode)\n\u001b[0;32m   1833\u001b[0m   handle\u001b[39m.\u001b[39m_parent_trackable \u001b[39m=\u001b[39m weakref\u001b[39m.\u001b[39mref(\u001b[39mself\u001b[39m)\n\u001b[0;32m   1834\u001b[0m \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:237\u001b[0m, in \u001b[0;36meager_safe_variable_handle\u001b[1;34m(initial_value, shape, shared_name, name, graph_mode)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Creates a variable handle with information to do shape inference.\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \n\u001b[0;32m    197\u001b[0m \u001b[39mThe dtype is read from `initial_value` and stored in the returned\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[39m  The handle, a `Tensor` of type `resource`.\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    236\u001b[0m dtype \u001b[39m=\u001b[39m initial_value\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mbase_dtype\n\u001b[1;32m--> 237\u001b[0m \u001b[39mreturn\u001b[39;00m _variable_handle_from_shape_and_dtype(shape, dtype, shared_name, name,\n\u001b[0;32m    238\u001b[0m                                              graph_mode, initial_value)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:161\u001b[0m, in \u001b[0;36m_variable_handle_from_shape_and_dtype\u001b[1;34m(shape, dtype, shared_name, name, graph_mode, initial_value)\u001b[0m\n\u001b[0;32m    156\u001b[0m     \u001b[39mraise\u001b[39;00m errors\u001b[39m.\u001b[39mInternalError(  \u001b[39m# pylint: disable=no-value-for-parameter\u001b[39;00m\n\u001b[0;32m    157\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUsing an explicit shared_name is not allowed when executing eagerly.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    158\u001b[0m     )\n\u001b[0;32m    159\u001b[0m   shared_name \u001b[39m=\u001b[39m context\u001b[39m.\u001b[39manonymous_name()\n\u001b[1;32m--> 161\u001b[0m handle \u001b[39m=\u001b[39m gen_resource_variable_ops\u001b[39m.\u001b[39;49mvar_handle_op(\n\u001b[0;32m    162\u001b[0m     shape\u001b[39m=\u001b[39;49mshape,\n\u001b[0;32m    163\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m    164\u001b[0m     shared_name\u001b[39m=\u001b[39;49mshared_name,\n\u001b[0;32m    165\u001b[0m     name\u001b[39m=\u001b[39;49mname,\n\u001b[0;32m    166\u001b[0m     container\u001b[39m=\u001b[39;49mcontainer)\n\u001b[0;32m    167\u001b[0m \u001b[39mif\u001b[39;00m initial_value \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    168\u001b[0m   initial_value \u001b[39m=\u001b[39m handle\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py:1226\u001b[0m, in \u001b[0;36mvar_handle_op\u001b[1;34m(dtype, shape, container, shared_name, allowed_devices, name)\u001b[0m\n\u001b[0;32m   1224\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[0;32m   1225\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1226\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[0;32m   1227\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mVarHandleOp\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, \u001b[39m\"\u001b[39;49m\u001b[39mcontainer\u001b[39;49m\u001b[39m\"\u001b[39;49m, container, \u001b[39m\"\u001b[39;49m\u001b[39mshared_name\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1228\u001b[0m       shared_name, \u001b[39m\"\u001b[39;49m\u001b[39mdtype\u001b[39;49m\u001b[39m\"\u001b[39;49m, dtype, \u001b[39m\"\u001b[39;49m\u001b[39mshape\u001b[39;49m\u001b[39m\"\u001b[39;49m, shape, \u001b[39m\"\u001b[39;49m\u001b[39mallowed_devices\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1229\u001b[0m       allowed_devices)\n\u001b[0;32m   1230\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m   1231\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train(epochs):\n",
    "  problems = 10\n",
    "  seconds_per_test = 4\n",
    "  last_test = time.time()\n",
    "  # For each epoch...\n",
    "  for epoch in range(epochs):\n",
    "    \n",
    "\n",
    "    # For each minibatch...\n",
    "    for _ in range(100):\n",
    "      input_data, output_data = generate_data(100)\n",
    "      train_step(input_data, output_data)\n",
    "    \n",
    "    # Test every n seconds\n",
    "    if time.time() - last_test >= seconds_per_test:\n",
    "      last_test = time.time()\n",
    "\n",
    "      s, w = evaluate(problems)\n",
    "      print (f\"Epoch {epoch + 1}: {s}/{problems}\")\n",
    "      print (f\"Wrongness: {w}\")\n",
    "\n",
    "      if s == problems:\n",
    "        print(f\"Hit 100% accuracy on {problems} problems after {epoch+1} epochs!\")\n",
    "        problems *= 10\n",
    "        seconds_per_test *= 4\n",
    "\n",
    "      print()\n",
    "\n",
    "  \n",
    "train(1000000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
