{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learns to solve simple math problems involving two ten-digit numbers.\n",
    "My goal at the beginning was to have it do multiplication.\n",
    "I had problems early on where it refused to converge at all. So I worked forwards starting on really easy problems.\n",
    "It had no trouble converging when the problem was, \"Take the first number and send it back out\". That confirmed that I at\n",
    "least had written the learning and question-generation correctly. I moved up to \"Take the first number and add three\".\n",
    "That worked too. Then I had it add a larger number (which would involve more carrying). I realized that the problem might\n",
    "be that I hadn't given it enough nodes to properly implement the logic. Once I upgraded the network from two 200-node\n",
    "hidden layers to 2000 and 500, it was able to solve the problem of adding the two numbers together with good accuracy. I\n",
    "still haven't gotten it to do multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mkeras\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\tensorflow\\__init__.py:469\u001b[0m\n\u001b[0;32m    467\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(_current_module, \u001b[39m\"\u001b[39m\u001b[39mkeras\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    468\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 469\u001b[0m     _keras\u001b[39m.\u001b[39;49m_load()\n\u001b[0;32m    470\u001b[0m   \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[0;32m    471\u001b[0m     \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\lazy_loader.py:41\u001b[0m, in \u001b[0;36mLazyLoader._load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Load the module and insert it into the parent's globals.\"\"\"\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[39m# Import the target module and insert it into the parent's namespace\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m module \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39;49mimport_module(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m)\n\u001b[0;32m     42\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parent_module_globals[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_local_name] \u001b[39m=\u001b[39m module\n\u001b[0;32m     44\u001b[0m \u001b[39m# Emit a warning if one was specified\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1002\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:945\u001b[0m, in \u001b[0;36m_find_spec\u001b[1;34m(name, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1439\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1411\u001b[0m, in \u001b[0;36m_get_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1544\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(self, fullname, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:147\u001b[0m, in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "NUM_SIZE = 10\n",
    "DIGITS = 4\n",
    "FIRST_LAYER_SIZE = 600\n",
    "EXTRA_LAYER_SIZE = 180\n",
    "EXTRA_LAYERS = DIGITS + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom layers\n",
    "\n",
    "# This is like the Softmax function, but it does it independently on each digit in the result\n",
    "# So each group of ten nodes should sum up to one\n",
    "class TenSoftmax(keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(TenSoftmax, self).__init__(**kwargs)\n",
    "        self.softmax = keras.layers.Softmax()\n",
    "    \n",
    "    def call(self, tensor, training=True):\n",
    "        s0, s1, s2, s3, s4, s5, s6, s7, s8, s9 = tf.split(tensor, num_or_size_splits=10, axis=1)\n",
    "        list = [s0, s1, s2, s3, s4, s5, s6, s7, s8, s9]\n",
    "        for i in range(10):\n",
    "            list[i] = self.softmax(list[1])\n",
    "        ret = tf.concat(list, axis=1)\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 1300)              261300    \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 1300)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 320)               416320    \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 320)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 320)               102720    \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 320)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 320)               102720    \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 320)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 320)               102720    \n",
      "                                                                 \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 320)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 320)               102720    \n",
      "                                                                 \n",
      " leaky_re_lu_5 (LeakyReLU)   (None, 320)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 320)               102720    \n",
      "                                                                 \n",
      " leaky_re_lu_6 (LeakyReLU)   (None, 320)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 320)               102720    \n",
      "                                                                 \n",
      " leaky_re_lu_7 (LeakyReLU)   (None, 320)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 320)               102720    \n",
      "                                                                 \n",
      " leaky_re_lu_8 (LeakyReLU)   (None, 320)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 320)               102720    \n",
      "                                                                 \n",
      " leaky_re_lu_9 (LeakyReLU)   (None, 320)               0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 320)               102720    \n",
      "                                                                 \n",
      " leaky_re_lu_10 (LeakyReLU)  (None, 320)               0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 320)               102720    \n",
      "                                                                 \n",
      " leaky_re_lu_11 (LeakyReLU)  (None, 320)               0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 100)               32100     \n",
      "                                                                 \n",
      " leaky_re_lu_12 (LeakyReLU)  (None, 100)               0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,736,920\n",
      "Trainable params: 1,736,920\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "def math_model():\n",
    "    # They happen in a linear order\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    # Input layer / First Hidden Layer\n",
    "    model.add(keras.layers.Dense(FIRST_LAYER_SIZE, input_shape=(2*10*NUM_SIZE,)))\n",
    "    model.add(keras.layers.LeakyReLU())\n",
    "\n",
    "    # Carry Layers\n",
    "    for _ in range(EXTRA_LAYERS):\n",
    "        model.add(keras.layers.Dense(EXTRA_LAYER_SIZE))\n",
    "        model.add(keras.layers.LeakyReLU())\n",
    "\n",
    "    # Output Layer\n",
    "    model.add(keras.layers.Dense(10*NUM_SIZE))\n",
    "    model.add(keras.layers.LeakyReLU())\n",
    "    # model.add(TenSoftmax())\n",
    "    \n",
    "    # Print summary\n",
    "    print(model.summary())\n",
    "\n",
    "    # Return\n",
    "    return model\n",
    "\n",
    "# Get the model\n",
    "model = math_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method returns a helper function to compute cross entropy loss\n",
    "loss_function = tf.keras.losses.MeanSquaredError()    \n",
    "# loss_function = tf.keras.losses.BinaryCrossentropy(from_logits=True)    \n",
    "# loss_function = tf.keras.losses.CategoricalCrossentropy()    \n",
    "\n",
    "# Declare optimizer (Use Adam optimizer w/ learning rate of 1e-4)\n",
    "optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(count):\n",
    "    input_data, output_data = generate_data(count)\n",
    "    output = model(input_data, training=True)\n",
    "\n",
    "    # Iterate over answers\n",
    "    correct_answers = 0\n",
    "    wrongness = 0\n",
    "    for yh, y in zip(output, output_data):\n",
    "        yhr = tf.reshape(yh, (10, 10))\n",
    "        yr = tf.reshape(y, (10, 10))\n",
    "        correct = True\n",
    "        for n1, n2 in zip(yhr, yr):\n",
    "            if np.argmax(n1) != np.argmax(n2):\n",
    "                correct = False\n",
    "                break\n",
    "        if correct:\n",
    "            correct_answers += 1\n",
    "\n",
    "    wrongness = loss_function(output, output_data)\n",
    "    \n",
    "    return correct_answers, wrongness\n",
    "\n",
    "def get_digit(number, digit):\n",
    "    number = number % (10**(digit+1))\n",
    "    number = int(number / (10**digit))\n",
    "    return number\n",
    "\n",
    "def digit_vector(num):\n",
    "    e = np.zeros([10])\n",
    "    e[num] = 1\n",
    "    return e\n",
    "\n",
    "def generate_data(data_points):\n",
    "    entries_input = []\n",
    "    entries_output = []\n",
    "    for i in range(data_points):\n",
    "        # Generate numbers\n",
    "        scale = 10**DIGITS\n",
    "        num1 = int(random.random() * scale)\n",
    "        num2 = int(random.random() * scale)\n",
    "        answer = num1 + num2\n",
    "\n",
    "        # Convert to vectors\n",
    "        entry_input = []\n",
    "        for i in range(9, -1, -1):\n",
    "            entry_input.extend(digit_vector(get_digit(num1, i)))\n",
    "        for i in range(9, -1, -1):\n",
    "            entry_input.extend(digit_vector(get_digit(num2, i)))\n",
    "        entry_output = []\n",
    "        for i in range(9, -1, -1):\n",
    "            entry_output.extend(digit_vector(get_digit(answer, i)))\n",
    "        \n",
    "        # Append\n",
    "        entries_input.append(entry_input)\n",
    "        entries_output.append(entry_output)\n",
    "    \n",
    "    ret_input = tf.Variable(entries_input, tf.float64)\n",
    "    ret_output = tf.Variable(entries_output, tf.float64)\n",
    "\n",
    "    # print (ret_input)\n",
    "    # print (ret_output)\n",
    "    \n",
    "    return ret_input, ret_output\n",
    "\n",
    "# generate_data(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.function annotation causes the function \n",
    "# to be \"compiled\" as part of the training\n",
    "@tf.function\n",
    "def train_step(input_data, output_data):\n",
    "    # Set up tape\n",
    "    with tf.GradientTape() as tape:\n",
    "      output = model(input_data, training=True)\n",
    "\n",
    "      loss = loss_function(output_data, output)\n",
    "\n",
    "      gradients = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "      optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    # Evaluate\n",
    "    #input_data_test, output_data_test = generate_data(100)\n",
    "    #output = model(input_data_test, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 0/10\n",
      "Wrongness: 0.035113328175621086\n",
      "\n",
      "Epoch 7: 0/10\n",
      "Wrongness: 0.032415875057890696\n",
      "\n",
      "Epoch 11: 0/10\n",
      "Wrongness: 0.03145170775030749\n",
      "\n",
      "Epoch 15: 0/10\n",
      "Wrongness: 0.031105232559521766\n",
      "\n",
      "Epoch 19: 0/10\n",
      "Wrongness: 0.028799574389958248\n",
      "\n",
      "Epoch 23: 0/10\n",
      "Wrongness: 0.02806650614907527\n",
      "\n",
      "Epoch 27: 0/10\n",
      "Wrongness: 0.02766656273280262\n",
      "\n",
      "Epoch 31: 0/10\n",
      "Wrongness: 0.028118929309650285\n",
      "\n",
      "Epoch 35: 0/10\n",
      "Wrongness: 0.027395685425850365\n",
      "\n",
      "Epoch 39: 0/10\n",
      "Wrongness: 0.028296756657634266\n",
      "\n",
      "Epoch 43: 0/10\n",
      "Wrongness: 0.0265668870808827\n",
      "\n",
      "Epoch 47: 0/10\n",
      "Wrongness: 0.02804431062769127\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 30\u001b[0m\n\u001b[0;32m     25\u001b[0m         seconds_per_test \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m \u001b[39m4\u001b[39m\n\u001b[0;32m     27\u001b[0m       \u001b[39mprint\u001b[39m()\n\u001b[1;32m---> 30\u001b[0m train(\u001b[39m1000000\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[8], line 11\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(epochs)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m      7\u001b[0m   \n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m   \u001b[39m# For each minibatch...\u001b[39;00m\n\u001b[0;32m     10\u001b[0m   \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m100\u001b[39m):\n\u001b[1;32m---> 11\u001b[0m     input_data, output_data \u001b[39m=\u001b[39m generate_data(\u001b[39m100\u001b[39;49m)\n\u001b[0;32m     12\u001b[0m     train_step(input_data, output_data)\n\u001b[0;32m     14\u001b[0m   \u001b[39m# Test every n seconds\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 57\u001b[0m, in \u001b[0;36mgenerate_data\u001b[1;34m(data_points)\u001b[0m\n\u001b[0;32m     54\u001b[0m     entries_input\u001b[39m.\u001b[39mappend(entry_input)\n\u001b[0;32m     55\u001b[0m     entries_output\u001b[39m.\u001b[39mappend(entry_output)\n\u001b[1;32m---> 57\u001b[0m ret_input \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mVariable(entries_input, tf\u001b[39m.\u001b[39;49mfloat64)\n\u001b[0;32m     58\u001b[0m ret_output \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mVariable(entries_output, tf\u001b[39m.\u001b[39mfloat64)\n\u001b[0;32m     60\u001b[0m \u001b[39m# print (ret_input)\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[39m# print (ret_output)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py:266\u001b[0m, in \u001b[0;36mVariableMetaclass.__call__\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    264\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_variable_v1_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    265\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m Variable:\n\u001b[1;32m--> 266\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_variable_v2_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    267\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    268\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m(VariableMetaclass, \u001b[39mcls\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py:247\u001b[0m, in \u001b[0;36mVariableMetaclass._variable_v2_call\u001b[1;34m(cls, initial_value, trainable, validate_shape, caching_device, name, variable_def, dtype, import_scope, constraint, synchronization, aggregation, shape)\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[39mif\u001b[39;00m aggregation \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    246\u001b[0m   aggregation \u001b[39m=\u001b[39m VariableAggregation\u001b[39m.\u001b[39mNONE\n\u001b[1;32m--> 247\u001b[0m \u001b[39mreturn\u001b[39;00m previous_getter(\n\u001b[0;32m    248\u001b[0m     initial_value\u001b[39m=\u001b[39;49minitial_value,\n\u001b[0;32m    249\u001b[0m     trainable\u001b[39m=\u001b[39;49mtrainable,\n\u001b[0;32m    250\u001b[0m     validate_shape\u001b[39m=\u001b[39;49mvalidate_shape,\n\u001b[0;32m    251\u001b[0m     caching_device\u001b[39m=\u001b[39;49mcaching_device,\n\u001b[0;32m    252\u001b[0m     name\u001b[39m=\u001b[39;49mname,\n\u001b[0;32m    253\u001b[0m     variable_def\u001b[39m=\u001b[39;49mvariable_def,\n\u001b[0;32m    254\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m    255\u001b[0m     import_scope\u001b[39m=\u001b[39;49mimport_scope,\n\u001b[0;32m    256\u001b[0m     constraint\u001b[39m=\u001b[39;49mconstraint,\n\u001b[0;32m    257\u001b[0m     synchronization\u001b[39m=\u001b[39;49msynchronization,\n\u001b[0;32m    258\u001b[0m     aggregation\u001b[39m=\u001b[39;49maggregation,\n\u001b[0;32m    259\u001b[0m     shape\u001b[39m=\u001b[39;49mshape)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py:240\u001b[0m, in \u001b[0;36mVariableMetaclass._variable_v2_call.<locals>.<lambda>\u001b[1;34m(**kws)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_variable_v2_call\u001b[39m(\u001b[39mcls\u001b[39m,\n\u001b[0;32m    227\u001b[0m                       initial_value\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    228\u001b[0m                       trainable\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    237\u001b[0m                       aggregation\u001b[39m=\u001b[39mVariableAggregation\u001b[39m.\u001b[39mNONE,\n\u001b[0;32m    238\u001b[0m                       shape\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    239\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Call on Variable class. Useful to force the signature.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 240\u001b[0m   previous_getter \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkws: default_variable_creator_v2(\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkws)\n\u001b[0;32m    241\u001b[0m   \u001b[39mfor\u001b[39;00m _, getter \u001b[39min\u001b[39;00m ops\u001b[39m.\u001b[39mget_default_graph()\u001b[39m.\u001b[39m_variable_creator_stack:  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    242\u001b[0m     previous_getter \u001b[39m=\u001b[39m _make_getter(getter, previous_getter)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py:2755\u001b[0m, in \u001b[0;36mdefault_variable_creator_v2\u001b[1;34m(next_creator, **kwargs)\u001b[0m\n\u001b[0;32m   2752\u001b[0m aggregation \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39maggregation\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m   2753\u001b[0m shape \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mshape\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m-> 2755\u001b[0m \u001b[39mreturn\u001b[39;00m resource_variable_ops\u001b[39m.\u001b[39;49mResourceVariable(\n\u001b[0;32m   2756\u001b[0m     initial_value\u001b[39m=\u001b[39;49minitial_value,\n\u001b[0;32m   2757\u001b[0m     trainable\u001b[39m=\u001b[39;49mtrainable,\n\u001b[0;32m   2758\u001b[0m     validate_shape\u001b[39m=\u001b[39;49mvalidate_shape,\n\u001b[0;32m   2759\u001b[0m     caching_device\u001b[39m=\u001b[39;49mcaching_device,\n\u001b[0;32m   2760\u001b[0m     name\u001b[39m=\u001b[39;49mname,\n\u001b[0;32m   2761\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m   2762\u001b[0m     constraint\u001b[39m=\u001b[39;49mconstraint,\n\u001b[0;32m   2763\u001b[0m     variable_def\u001b[39m=\u001b[39;49mvariable_def,\n\u001b[0;32m   2764\u001b[0m     import_scope\u001b[39m=\u001b[39;49mimport_scope,\n\u001b[0;32m   2765\u001b[0m     distribute_strategy\u001b[39m=\u001b[39;49mdistribute_strategy,\n\u001b[0;32m   2766\u001b[0m     synchronization\u001b[39m=\u001b[39;49msynchronization,\n\u001b[0;32m   2767\u001b[0m     aggregation\u001b[39m=\u001b[39;49maggregation,\n\u001b[0;32m   2768\u001b[0m     shape\u001b[39m=\u001b[39;49mshape)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py:268\u001b[0m, in \u001b[0;36mVariableMetaclass.__call__\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    266\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_variable_v2_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    267\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 268\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m(VariableMetaclass, \u001b[39mcls\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:1659\u001b[0m, in \u001b[0;36mResourceVariable.__init__\u001b[1;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy, synchronization, aggregation, shape)\u001b[0m\n\u001b[0;32m   1656\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_from_proto(variable_def, import_scope\u001b[39m=\u001b[39mimport_scope,\n\u001b[0;32m   1657\u001b[0m                         validate_shape\u001b[39m=\u001b[39mvalidate_shape)\n\u001b[0;32m   1658\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1659\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_init_from_args(\n\u001b[0;32m   1660\u001b[0m       initial_value\u001b[39m=\u001b[39;49minitial_value,\n\u001b[0;32m   1661\u001b[0m       trainable\u001b[39m=\u001b[39;49mtrainable,\n\u001b[0;32m   1662\u001b[0m       collections\u001b[39m=\u001b[39;49mcollections,\n\u001b[0;32m   1663\u001b[0m       caching_device\u001b[39m=\u001b[39;49mcaching_device,\n\u001b[0;32m   1664\u001b[0m       name\u001b[39m=\u001b[39;49mname,\n\u001b[0;32m   1665\u001b[0m       dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m   1666\u001b[0m       constraint\u001b[39m=\u001b[39;49mconstraint,\n\u001b[0;32m   1667\u001b[0m       synchronization\u001b[39m=\u001b[39;49msynchronization,\n\u001b[0;32m   1668\u001b[0m       aggregation\u001b[39m=\u001b[39;49maggregation,\n\u001b[0;32m   1669\u001b[0m       shape\u001b[39m=\u001b[39;49mshape,\n\u001b[0;32m   1670\u001b[0m       distribute_strategy\u001b[39m=\u001b[39;49mdistribute_strategy,\n\u001b[0;32m   1671\u001b[0m       validate_shape\u001b[39m=\u001b[39;49mvalidate_shape,\n\u001b[0;32m   1672\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:1827\u001b[0m, in \u001b[0;36mResourceVariable._init_from_args\u001b[1;34m(self, initial_value, trainable, collections, caching_device, name, dtype, constraint, synchronization, aggregation, distribute_strategy, shape, validate_shape)\u001b[0m\n\u001b[0;32m   1825\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1826\u001b[0m     shape \u001b[39m=\u001b[39m initial_value\u001b[39m.\u001b[39mshape\n\u001b[1;32m-> 1827\u001b[0m   handle \u001b[39m=\u001b[39m eager_safe_variable_handle(\n\u001b[0;32m   1828\u001b[0m       initial_value\u001b[39m=\u001b[39;49minitial_value,\n\u001b[0;32m   1829\u001b[0m       shape\u001b[39m=\u001b[39;49mshape,\n\u001b[0;32m   1830\u001b[0m       shared_name\u001b[39m=\u001b[39;49mshared_name,\n\u001b[0;32m   1831\u001b[0m       name\u001b[39m=\u001b[39;49mname,\n\u001b[0;32m   1832\u001b[0m       graph_mode\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_in_graph_mode)\n\u001b[0;32m   1833\u001b[0m   handle\u001b[39m.\u001b[39m_parent_trackable \u001b[39m=\u001b[39m weakref\u001b[39m.\u001b[39mref(\u001b[39mself\u001b[39m)\n\u001b[0;32m   1834\u001b[0m \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:237\u001b[0m, in \u001b[0;36meager_safe_variable_handle\u001b[1;34m(initial_value, shape, shared_name, name, graph_mode)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Creates a variable handle with information to do shape inference.\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \n\u001b[0;32m    197\u001b[0m \u001b[39mThe dtype is read from `initial_value` and stored in the returned\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[39m  The handle, a `Tensor` of type `resource`.\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    236\u001b[0m dtype \u001b[39m=\u001b[39m initial_value\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mbase_dtype\n\u001b[1;32m--> 237\u001b[0m \u001b[39mreturn\u001b[39;00m _variable_handle_from_shape_and_dtype(shape, dtype, shared_name, name,\n\u001b[0;32m    238\u001b[0m                                              graph_mode, initial_value)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:161\u001b[0m, in \u001b[0;36m_variable_handle_from_shape_and_dtype\u001b[1;34m(shape, dtype, shared_name, name, graph_mode, initial_value)\u001b[0m\n\u001b[0;32m    156\u001b[0m     \u001b[39mraise\u001b[39;00m errors\u001b[39m.\u001b[39mInternalError(  \u001b[39m# pylint: disable=no-value-for-parameter\u001b[39;00m\n\u001b[0;32m    157\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUsing an explicit shared_name is not allowed when executing eagerly.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    158\u001b[0m     )\n\u001b[0;32m    159\u001b[0m   shared_name \u001b[39m=\u001b[39m context\u001b[39m.\u001b[39manonymous_name()\n\u001b[1;32m--> 161\u001b[0m handle \u001b[39m=\u001b[39m gen_resource_variable_ops\u001b[39m.\u001b[39;49mvar_handle_op(\n\u001b[0;32m    162\u001b[0m     shape\u001b[39m=\u001b[39;49mshape,\n\u001b[0;32m    163\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m    164\u001b[0m     shared_name\u001b[39m=\u001b[39;49mshared_name,\n\u001b[0;32m    165\u001b[0m     name\u001b[39m=\u001b[39;49mname,\n\u001b[0;32m    166\u001b[0m     container\u001b[39m=\u001b[39;49mcontainer)\n\u001b[0;32m    167\u001b[0m \u001b[39mif\u001b[39;00m initial_value \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    168\u001b[0m   initial_value \u001b[39m=\u001b[39m handle\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py:1226\u001b[0m, in \u001b[0;36mvar_handle_op\u001b[1;34m(dtype, shape, container, shared_name, allowed_devices, name)\u001b[0m\n\u001b[0;32m   1224\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[0;32m   1225\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1226\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[0;32m   1227\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mVarHandleOp\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, \u001b[39m\"\u001b[39;49m\u001b[39mcontainer\u001b[39;49m\u001b[39m\"\u001b[39;49m, container, \u001b[39m\"\u001b[39;49m\u001b[39mshared_name\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1228\u001b[0m       shared_name, \u001b[39m\"\u001b[39;49m\u001b[39mdtype\u001b[39;49m\u001b[39m\"\u001b[39;49m, dtype, \u001b[39m\"\u001b[39;49m\u001b[39mshape\u001b[39;49m\u001b[39m\"\u001b[39;49m, shape, \u001b[39m\"\u001b[39;49m\u001b[39mallowed_devices\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1229\u001b[0m       allowed_devices)\n\u001b[0;32m   1230\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m   1231\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train(epochs):\n",
    "  problems = 10\n",
    "  seconds_per_test = 4\n",
    "  last_test = time.time()\n",
    "  # For each epoch...\n",
    "  for epoch in range(epochs):\n",
    "    \n",
    "\n",
    "    # For each minibatch...\n",
    "    for _ in range(100):\n",
    "      input_data, output_data = generate_data(100)\n",
    "      train_step(input_data, output_data)\n",
    "    \n",
    "    # Test every n seconds\n",
    "    if time.time() - last_test >= seconds_per_test:\n",
    "      last_test = time.time()\n",
    "\n",
    "      s, w = evaluate(problems)\n",
    "      print (f\"Epoch {epoch + 1}: {s}/{problems}\")\n",
    "      print (f\"Wrongness: {w}\")\n",
    "\n",
    "      if s == problems:\n",
    "        print(f\"Hit 100% accuracy on {problems} problems after {epoch+1} epochs!\")\n",
    "        problems *= 10\n",
    "        seconds_per_test *= 4\n",
    "\n",
    "      print()\n",
    "\n",
    "  \n",
    "train(1000000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
